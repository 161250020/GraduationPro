'''
分词函数，按照空格进行分词
'''
def words_tokenize(text):
    return text.split(' ')